{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "# Autoencoder - Tensorflow 2.0 with eager mode and graph mode\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to generate images of handwritten digits using both eager execution and graph mode by training an Autoencoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "! pip install -q tensorflow-gpu==2.0.0-beta1\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q8-utnieNzHw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Eh-iCRVBm0p"
   },
   "source": [
    "## Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yC0rqzaWNzH5"
   },
   "outputs": [],
   "source": [
    "def standard_scale(X_train, X_test):\n",
    "    preprocessor = prep.StandardScaler().fit(X_train)\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpBo8BR8NzH_"
   },
   "outputs": [],
   "source": [
    "def get_random_block_from_data(data, batch_size):\n",
    "    start_index = np.random.randint(0, len(data) - batch_size)\n",
    "    return data[start_index:(start_index + batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8OMS1JONzIG"
   },
   "source": [
    "## Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-6nUMUlNzII"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    '''Encodes a digit from the MNIST dataset'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                n_dims,\n",
    "                name='encoder',\n",
    "                **kwargs):\n",
    "        super(Encoder,self).__init__(name=name, **kwargs)\n",
    "        self.n_dims = n_dims\n",
    "        self.n_layers = 1\n",
    "        self.encode_layer = layers.Dense(n_dims, activation='relu')\n",
    "        \n",
    "    @tf.function        \n",
    "    def call(self, inputs):\n",
    "        return self.encode_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4YmUVd8NzIO"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    '''Decodes a digit from the MNIST dataset'''\n",
    "\n",
    "    def __init__(self,\n",
    "                n_dims,\n",
    "                name='decoder',\n",
    "                **kwargs):\n",
    "        super(Decoder,self).__init__(name=name, **kwargs)\n",
    "        self.n_dims = n_dims\n",
    "        self.n_layers = len(n_dims)\n",
    "        self.decode_middle = layers.Dense(n_dims[0], activation='relu')\n",
    "        self.recon_layer = layers.Dense(n_dims[1], activation='sigmoid')\n",
    "        \n",
    "    @tf.function        \n",
    "    def call(self, inputs):\n",
    "        x = self.decode_middle(inputs)\n",
    "        return self.recon_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqCLs1UoNzIX"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    '''Vanilla Autoencoder for MNIST digits'''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 n_dims=[200, 392, 784],\n",
    "                 name='autoencoder',\n",
    "                 **kwargs):\n",
    "        super(Autoencoder, self).__init__(name=name, **kwargs)\n",
    "        self.n_dims = n_dims\n",
    "        self.encoder = Encoder(n_dims[0])\n",
    "        self.decoder = Decoder([n_dims[1], n_dims[2]])\n",
    "        \n",
    "    @tf.function        \n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEQGqMzyNzIe"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5PSeedlNzIh"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHcU9qpwNzIp"
   },
   "outputs": [],
   "source": [
    "(X_train, _), (X_test, _) = mnist.load_data()\n",
    "X_train = tf.cast(np.reshape(X_train, (X_train.shape[0], X_train.shape[1] * X_train.shape[2])), tf.float64)\n",
    "X_test = tf.cast(np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2])), tf.float64)\n",
    "\n",
    "X_train, X_test = standard_scale(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j5pW1MK6NzIv"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(X_train).batch(128).shuffle(buffer_size=1024)\n",
    "test_data = tf.data.Dataset.from_tensor_slices(X_test).batch(128).shuffle(buffer_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8sDlNzkNzI0"
   },
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duilfQyHNzI1"
   },
   "outputs": [],
   "source": [
    "n_samples = int(len(X_train) + len(X_test))\n",
    "training_epochs = 20\n",
    "batch_size = 128\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azYo2QWgNzI6"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "loss_metric = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huIJC7iHNzI-"
   },
   "source": [
    "## Eager mode training\n",
    "Using `tf.GradientTape()` to expose the tape to perform Autodifferentiation. Tensorflow \"records\" all operations executed inside the context of a \"tape\". Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using reverse mode differentiation. The optimizer can then be used to apply the gradients to all the trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wA9huq7ZNzJA"
   },
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder([200, 394, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTqNHWmwNzJE"
   },
   "outputs": [],
   "source": [
    "# Iterate over epochs.\n",
    "for epoch in range(5):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "    for step, x_batch in enumerate(train_data):\n",
    "        with tf.GradientTape() as tape:\n",
    "          recon = autoencoder(x_batch)\n",
    "          loss = mse_loss(x_batch, recon)\n",
    "\n",
    "        grads = tape.gradient(loss, autoencoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_variables))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "          print(f'Step {step}: mean loss = {loss_metric.result()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lO4Fh-3iNzJJ"
   },
   "source": [
    "## Graph mode training\n",
    "Graph mode training with `tf.keras.Model.fit` creates a computation graph and fits the training data on the keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1LvHDCXNzJL"
   },
   "outputs": [],
   "source": [
    "ae = Autoencoder([200, 392, 784])\n",
    "ae.compile(optimizer=tf.optimizers.Adam(0.01), loss='categorical_crossentropy')\n",
    "ae.fit(X_train, X_train, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhNtHfuxCGVy"
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrsKXcPRUvK9"
   },
   "source": [
    "- Eager Execution Basics [TF Docs Link](https://www.tensorflow.org/tutorials/eager/eager_basics)\n",
    "- Tensorflow Automatic Differentiation [TF Docs Link](https://www.tensorflow.org/tutorials/eager/automatic_differentiation)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "autoencoder.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
