{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tiny_imagenet_custom_resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tuOe1ymfHZPu",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "# TinyImageNet Custom ResNet\n",
        "\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/template/notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "{TODO: Fill in detailed info of what this accomplishes}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNbeBoBWsDfa",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBluBf7GsDfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "print(f'{tf.__version__}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnv2z7-O2clU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/Vishal-V/GSoC.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxsZAxRH4eIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from load_data import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLbg3owpsDfg",
        "colab_type": "text"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT8LP0hfsDfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "download_path = os.getcwd()\n",
        "    \n",
        "import pathlib\n",
        "path = tf.keras.utils.get_file('tiny-imagenet-200.zip', extract=True, cache_subdir=download_path,\n",
        "                               origin='http://cs231n.stanford.edu/tiny-imagenet-200.zip')\n",
        "\n",
        "data_dir = pathlib.Path(path).with_suffix('')\n",
        "\n",
        "TRAIN = data_dir/\"train\"\n",
        "VAL = data_dir/\"val/images\"\n",
        "VAL_ANNOT = data_dir/'val/val_annotations.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-SQU5PLsDfn",
        "colab_type": "text"
      },
      "source": [
        "## Image augmentation and image generators\n",
        "- The function below returns the generators for the ImageDataGenerator objects we will use to train and validate our ResNet model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGSBMQVqsDfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "val_data = pd.read_csv(VAL_ANNOT , sep='\\t', names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
        "val_data.drop(['X','Y','H', 'W'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "def train_val_gen(train_target=64, train_batch=64, val_target=64, val_batch=64):\n",
        "\t\t\n",
        "\t\ttrain_datagen = ImageDataGenerator(\n",
        "\t\t        rescale=1./255,\n",
        "\t\t        rotation_range=18, # Rotation Angle\n",
        "\t\t        zoom_range=0.15,  # Zoom Range\n",
        "\t\t        width_shift_range=0.2, # Width Shift\n",
        "\t\t        height_shift_range=0.2, # Height Shift\n",
        "\t\t        shear_range=0.15,  # Shear Range\n",
        "\t\t        horizontal_flip=True, # Horizontal Flip\n",
        "\t\t        fill_mode=\"reflect\", # Fills empty with reflections\n",
        "\t\t        brightness_range=[0.4, 1.6]  # Increasing/decreasing brightness\n",
        "\t\t)\n",
        "\n",
        "\t\ttrain_generator = train_datagen.flow_from_directory(\n",
        "\t\t        TRAIN,\n",
        "\t\t        target_size=(train_target, train_target),\n",
        "\t\t        batch_size=train_batch,\n",
        "\t\t        class_mode='categorical')\n",
        "\n",
        "\t\tval_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\t\tval_generator = val_datagen.flow_from_dataframe(\n",
        "\t\t    val_data, directory=VAL, \n",
        "\t\t    x_col='File', \n",
        "\t\t    y_col='Class', \n",
        "\t\t    target_size=(val_target, val_target),\n",
        "\t\t    color_mode='rgb', \n",
        "\t\t    class_mode='categorical', \n",
        "\t\t    batch_size=val_batch, \n",
        "\t\t    shuffle=False, \n",
        "\t\t    seed=42\n",
        "\t\t)\n",
        "\n",
        "\t\treturn train_generator, val_generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlmlEVva1lgv",
        "colab_type": "text"
      },
      "source": [
        "## Defining callbacks to employ different training strategies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSzYk0oH10mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhrD1MEvsDft",
        "colab_type": "text"
      },
      "source": [
        "## Custom ResNet that uses Pre-Activation and BottleNeck Blocks with SeparableConv2D\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "-  We use 1x1 to increase the number of channels is to create a wider model with minimum increase in trainable parameters\n",
        "- This [reserach paper](https://arxiv.org/abs/1812.01187) documents improved accuracy with AveragePooling2D in the shortcut connection. This model showed a performance drop and hence was replaced with a 1x1 convolution.\n",
        "- Uses SeparableConv2D rather than vanilla Conv2D to reduce the nmber of parameters and make the model feasible to train on constrained environments like colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgNW09MOsDfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, SeparableConv2D, AveragePooling2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import ZeroPadding2D, Activation, Flatten, add\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNdKeWjwsDfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet:\n",
        "    \n",
        "    def residual_block(data, K, stride, chanDim, red=False, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
        "        shortcut = data\n",
        "\n",
        "        bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(data)\n",
        "        act1 = Activation(\"relu\")(bn1)\n",
        "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "        bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv1)\n",
        "        act2 = Activation(\"relu\")(bn2)\n",
        "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride, padding=\"same\", use_bias=False, kernel_regularizer=l2(reg))(act2)\n",
        "\n",
        "        bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(conv2)\n",
        "        act3 = Activation(\"relu\")(bn3)\n",
        "        conv3 = Conv2D(K, (1, 1), use_bias=False, kernel_regularizer=l2(reg))(act3)\n",
        "\n",
        "        if red:\n",
        "            shortcut = Conv2D(K, (1, 1), strides=stride, use_bias=False, kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "        x = add([conv3, shortcut])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def build(width, height, depth, classes, stages, filters, reg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
        "        inputShape = (height, width, depth)\n",
        "        chanDim = -1\n",
        "\n",
        "        inputs = tf.keras.Input(shape=inputShape)\n",
        "        x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(inputs)\n",
        "\n",
        "        x = Conv2D(filters[0], (5, 5), use_bias=False, padding=\"same\", kernel_regularizer=l2(reg))(x)\n",
        "        x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = ZeroPadding2D((1, 1))(x)\n",
        "        x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "    \n",
        "        for i in range(0, len(stages)):\n",
        "            stride = (1, 1) if i == 0 else (2, 2)\n",
        "            x = ResNet.residual_block(x, filters[i + 1], stride, chanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "            for j in range(0, stages[i] - 1):\n",
        "                x = ResNet.residual_block(x, filters[i + 1], (1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
        "\n",
        "        x = BatchNormalization(axis=chanDim, epsilon=bnEps, momentum=bnMom)(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = AveragePooling2D((8, 8))(x)\n",
        "        x = Conv2D(200, (1,1), kernel_regularizer=l2(reg))(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Activation(\"softmax\")(x)\n",
        "\n",
        "        model = tf.keras.Model(inputs, x, name=\"resnet\")\n",
        "\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKVErJnbsDf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ResNet.build(64, 64, 3, 200, (3, 4, 6), (64, 128, 256, 512), reg=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD15cK08sDf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9GdQWfCsDgE",
        "colab_type": "text"
      },
      "source": [
        "## Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kib6v8vhsDgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, epsilon=0.1, amsgrad=False)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rwwKT2EsDgO",
        "colab_type": "text"
      },
      "source": [
        "## Using fit_generator to train the model\n",
        "ImageDataGenerator is best suited for augmenting images on the fly and training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztTE-5SJ83jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gen, val_gen = train_val_gen(train_target=64, train_batch=64, val_target=64, val_batch=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_paZU0BAsDgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "  train_gen,\n",
        "  steps_per_epoch=100000 // 64,\n",
        "  validation_data=val_gen,\n",
        "  validation_steps=10000 // 64,\n",
        "  epochs=20,\n",
        "  max_queue_size=64,\n",
        "  verbose=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YytXkT3CLYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwZkcCnEsDgU",
        "colab_type": "text"
      },
      "source": [
        "### List of references for easy lookup\n",
        "\n",
        "---\n",
        "\n",
        "1. Building blocks of interpretability: [Link](https://distill.pub/2018/building-blocks/) (Holy Grail of Intuition!)\n",
        "2. Deep Residual Learning for image classification: [Link](https://arxiv.org/abs/1512.03385) (Resnet Paper)\n",
        "3. Bag of tricks for image classification: [Link](https://arxiv.org/abs/1812.01187) (Tweaks and tricks to Resnet for increased performance paper)\n",
        "2. Imbalanced Deep Learning by Minority Class\n",
        "Incremental Rectification: [Link](https://arxiv.org/pdf/1804.10851.pdf) (Selectively Sampling Data paper)\n",
        "2. Improved Regularization of Convolutional Neural Networks with Cutout: [Link](https://arxiv.org/pdf/1708.04552.pdf) (Cutout/Occlusion Augmentation paper)\n",
        "3. Survey of resampling techniques for improving\n",
        "classification performance in unbalanced datasets [Link](https://arxiv.org/pdf/1608.06048v1.pdf) (Resampling paper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UhNtHfuxCGVy"
      },
      "source": [
        "## Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKhmFeraTdEI"
      },
      "source": [
        "For general instructions on how to write docs for Tensorflow see [Writing TensorFlow Documentation](https://www.tensorflow.org/community/documentation).\n",
        "\n",
        "The tips below are specific to notebooks for tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2V22fKegUtF9"
      },
      "source": [
        "### General\n",
        "\n",
        "* Include the collapsed license at the top (this uses Colab's \"Form\" mode to hide the cells).\n",
        "* Only include a single `H1` title.\n",
        "* Include the button-bar immediately under the `H1`.\n",
        "* Include an overview section before any code.\n",
        "* Put all your installs and imports in a setup section.\n",
        "* Always include the three `__future__` imports.\n",
        "* Save the notebook with the Table of Contents open.\n",
        "* Write python3 compatible code.\n",
        "* Keep cells small (~max 20 lines).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrsKXcPRUvK9"
      },
      "source": [
        "### Working in GitHub\n",
        "\n",
        "* Be consistent about how you save your notebooks, otherwise the JSON-diffs will be a mess.\n",
        "\n",
        "* This notebook has the \"Omit code cell output when saving this notebook\" option set. GitHub refuses to diff notebooks with large diffs (inline images).\n",
        "\n",
        "* [reviewnb.com](http://reviewnb.com) may help. You can access it using this bookmarklet:\n",
        "\n",
        "  ```\n",
        "javascript:(function(){ window.open(window.location.toString().replace(/github\\.com/, 'app.reviewnb.com').replace(/files$/,\"\")); })()\n",
        " ```\n",
        " \n",
        "* To open a GitHub notebook in Colab use the [Open in Colab](https://chrome.google.com/webstore/detail/open-in-colab/iogfkhleblhcpcekbiedikdehleodpjo) extension (or make a bookmarklet).\n",
        "  \n",
        "* The easiest way to edit a notebook in GitHub is to open it with Colab from the branch you want to edit. Then use File --> Save a copy in GitHub, which will save it back to the branch you opened it from.\n",
        "\n",
        "* For PRs it's helpful to post a direct Colab link to the PR head: https://colab.research.google.com/github/{user}/{repo}/blob/{branch}/{path}.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QKp40qS-DGEZ"
      },
      "source": [
        "### Code Style\n",
        "\n",
        "\n",
        "* Notebooks are for people. Write code optimized for clarity.\n",
        "\n",
        "* Demonstrate small parts before combining them into something more complex. Like below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KtylpxOmceaC",
        "colab": {}
      },
      "source": [
        "#Build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(None, 5)),\n",
        "    tf.keras.layers.Dense(3)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mMOeXVmbdilM",
        "colab": {}
      },
      "source": [
        "# Run the model on a single batch of data, and inspect the output.\n",
        "result = model(tf.constant(np.random.randn(10,5), dtype = tf.float32)).numpy()\n",
        "\n",
        "print(\"min:\", result.min())\n",
        "print(\"max:\", result.max())\n",
        "print(\"mean:\", result.mean())\n",
        "print(\"shape:\", result.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U82B_tH2d294",
        "colab": {}
      },
      "source": [
        "# Compile the model for training\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.categorical_crossentropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g3-lzxbCZi-H"
      },
      "source": [
        "* Keep examples quick. Use small datasets, or small slices of datasets. You don't need to train to convergence, train until it's obvious it's making progress.\n",
        "\n",
        "* For a large example, don't try to fit all the code in the notebook. Add python files to tensorflow examples, and in the noptebook run: `!pip install git+https://github.com/tensorflow/examples`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TJdqBNBbS78n"
      },
      "source": [
        "### Code content\n",
        "\n",
        "Use the highest level API that gets the job done (unless the goal is to demonstrate the low level API).\n",
        "\n",
        "Use `keras.Sequential` > keras functional api > keras model subclassing > ...\n",
        "\n",
        "Use  `model.fit` > `model.train_on_batch` > manual `GradientTapes`.\n",
        "\n",
        "Use eager-style code.\n",
        "\n",
        "Use `tensorflow_datasets` and `tf.data` where possible.\n",
        "\n",
        "Avoid `compat.v1`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "78HBT9cQXJko"
      },
      "source": [
        "### Text\n",
        "\n",
        "* Use an imperative style. \"Run a batch of images through the model.\"\n",
        "\n",
        "* Use sentence case in titles/headings. \n",
        "\n",
        "* Use short titles/headings: \"Download the data\", \"Build the Model\", \"Train the model\".\n",
        "\n"
      ]
    }
  ]
}